%------------------------------------------------------------
% Description : Matstatistics 
% Author      : taxus-d <iliya.t@mail.ru>
% Created at  : Thu Jan 12 14:26:44 MSK 2017
%------------------------------------------------------------
\documentclass[12pt,timbord]{../../../notes}
\usepackage{silence}
\WarningFilter{latex}{Reference}
\graphicspath{{../../img/}}

\begin{document}

\paragraph{Случайные векторы}
\label{par:stat::randvec}


\begin{defn}[Случайный вектор]\label{defn:prob::randvec::randscal}
  Случайным вектором назовём произвольное хорошее отображение $X\colon \Omega \to \R^n$.
  См. примечание про измеримость в \ref{par:prob::randscal}. Борелевские множества можно рассматривать 
  и в $\R^n$, как наименьшую сигма-алгебру, содержащую все полуоткрытые параллелепипеды.

  Так же можно считать, что случайный вектор~--- набор случайных величин.
\end{defn}


\begin{defn}[Функция распределения]\label{defn:prob::randvec::distrfun}
  \[
    F_X\colon \R^n\to [0;1] \colon\; F_X(x) = P(X^1 < x^1, \dotsc, X^n < x^n) 
  \]
  То есть вероятность попадания в параллелепипед, уходящий в бесконечность.
\end{defn}
\begin{rem}\label{rem:stat::randvec::distr}
Тут как раз используется, что борелевские множества <<прямоугольные>>.
\end{rem}


\begin{prop}\label{prop:prob::randscal::distrfun}
  Про $F(x)$ верно следущее:
  \begin{enumerate}
    \item $F$ не убывает по каждому аргументу.
    \item $\displaystyle\lim_{x_i\to -\infty}F(x) = 0$
    \item $\displaystyle\lim_{x\to +\infty}F(x) = 1$
    \item $\displaystyle\lim_{x\to x_0-0}F(x) = F(x_0)$ (по совокупности переменных).
      Это просто следует из непрерывности меры.
  \end{enumerate}
\end{prop}

тоже важно, так что отдельно

\begin{prop}\label{prop:stat::randvec::inc}
  Пусть $a^1 < b^1, \dotsc, a^n < b^n$, тогда работает формула включений и исключений
  \[
    F(b^1, \dotsc, b^n) - \sum_i F(b^1, \dotsc, a^i, \dotsc, b^n) + \dotsb + F(a^1, \dotsc, a^n) = 
  P(x \in [a^1, b^1)\times[a^n,b^n))
  \]
  По сути следствие формулки про вероятность объединения.
\end{prop}


\begin{defn}\label{defn:stat::randvec::disc}
  Векторная случайная величина называется дискретной, если
  \[
    \exists\,(\{a_i\mid a_i \in \R^n \} \sim \N)\colon \left( \sum_{i} P(X = a_i) = 1 \right)
  \]
  то есть 
  \[
    P(X\in B) = \sum_{\{i\mid a_i\in B\}} p_i, \; p_i = P(X = a_i)
  \]
\end{defn}
\begin{defn}\label{defn:stat::randvec::cony}
  Векторная случайная величина называется непрерывной, если
  \[
    \exists\,(f_X\colon B\to \R )\colon \left( P(X\in B) = \int_{B} f_X(x^1, \dotsc, x^n)\, \del
    x^1 \dotsm \del x^n \right)
  \]
\end{defn}
\begin{rem}\label{rem:stat::randvec::distfun }
  Для функций распределения:
  \[
    F(x^1, \dotsc, x^n) = \int_{-\infty}^{x^n}\dotsi\int_{-\infty}^{x^1} f_X(x^1, \dotsc, x^n)\,
    \del x^1 \dotsm \del x^n 
  \]
\end{rem}

\begin{prop}\label{prop:stat::randvec::ind}
  Пусть $X, Y$~--- независимы. Тогда $p_{X+Y}(x,y)=p_X(x) \cdot p_Y(y)$
\end{prop}
\begin{itlproof}
  По определению функции распределения
  \[
    F_{X,Y}(x,y) = \int_{-\infty}^x\int_{-\infty}^y p(x,y) \, \del x\del y
  \]
  Из независимости $X,Y$
  \[
    \begin{split}
      F_{X,Y} (x,y) &= P(X<x,Y<y) = P({\omega\mid X(\omega)\in (-\infty;x]}\cap {\omega\mid
        Y(\omega)\in (-\infty;y]} \\
        &= P({\omega\mid X(\omega)\in (-\infty;x]})\cdot
        P({\omega\mid Y(\omega)\in (-\infty;y]}) \\
        &= P(X<x) \cdot P(Y<y) = F_X(x)\cdot F_Y(y)
      \end{split}
  \]
  А тогда из независимости подынтегральных функций
  \[
    F_X(x)\cdot F_Y(y) = \int_{-\infty}^x p_X(x) \, \del x \cdot \int_{-\infty}^y p_Y(y) \, \del y 
    = \int_{-\infty}^x\int_{-\infty}^y p_X(x)\cdot p_Y(y) \, \del x\del y
  \]
  А дальше можно заметить, что нам неважно по какому множеству интегрировать.
  \[
    \int_B (p(x,y) - p_X(x)\cdot p_Y(y)) \, \del x\del y = 0
  \]
  Здесь правда всё ломается на отсутствии непрерывности у $p$. Но если она есть, то дальше
  стандартное рассуждение в окрестности точки где не 0.
\end{itlproof}

\paragraph{Функция от случайного вектора}
\label{par:stat::funcrand}

\begin{defn}[Функция от случайного вектора]\label{defn:stat::funcrand::funcrand}
  $g \colon \R^n \to \R^m$. 
  \begin{itaux}
    Здесь нужно снова говорить про измеримость $g$~--- прообраз борелевского множества должен
    быть борелевским множеством. Иначе $g(X)$ может не получиться случайной величиной. Но всякие
    мерзкие отображения всё равно никому не нужны $\ddot\smile$.
  \end{itaux}
\end{defn}

\begin{prop}\label{prop:stat::funcrand::disc}
  Пусть $X$~--- дискретная случайная величина, $f$~--- обратима, $Y=g(X)$, $b_j= f(a_j)$.
  Тогда $P(Y^i=b^i_j) =P(X^i=a^i_j)$.
\end{prop}
\begin{itlproof}
  \[
  P(Y^i=b^i_j) = P(f(X^i)= f(a^i_j)) = P({\omega \mid f(X^i(\omega))=f(a^i_j)}) 
  \]
  Поскольку $f$~--- обратима, она биективна. Значит $f(X) = f(a_j) \Leftrightarrow X = a_j$.
  Собственно, всё.
\end{itlproof}
\begin{prop}\label{prop:stat::funcrand::cont}
  Пусть $X$~--- непрерывная случайная величина, $f$~--- обратима, $Y=g(X), f^{-1} = g$.
  Тогда $p_y(y) = p_X(g(y)) \left| \frac{\del g}{\del y} \right|$ .
\end{prop}
\begin{itlproof}
  Пусть $D=f(B)$. Тогда $P(Y \in D) = P(X \in B)$ опять-таки в силу биективности $f$. Ну, ничего
  нового туда попасть не может и у всего есть  прообраз. Так что (здесь будем рисовать один значок
  интеграла и дифференциала из экономии размера пдф-ки, хотя в этом замечании данных может и больше)
  \begin{align*}
    \int_D p_Y(y) \, \del y  = \int_B  p_X(x) \del x 
    = \int_D  p_X(g(y)) \left| \frac{\del g}{\del y} \right|  
  \end{align*}
  Якобиан тут под модулем, так как множество неориентированное. Я верю, что нам ещё про это
  расскажут на матане.
\end{itlproof}

\paragraph{Матожидание и дисперсия суммы случайных величин}
\label{par:stat::randsum}

\begin{prop}\label{prop:stat::randsum::discsum}
  Пусть $X,Y,n\in(A\sim\N)$~--- две дискретные независимые случайные величины. Тогда 
  \[
    P(X+Y=n) = \sum_{k\in A} P(X=n-k)\cdot P(Y=k)
  \]
\end{prop}
\begin{itlproof}
  Из формулы полной вероятности ($Y=k, k\in A$ правда полная группа)
  \[
    P(X+Y =n) = \sum_{k\in A} P(X+Y =n \mid Y =k)\cdot P(Y = k) 
    = \sum_{k\in A} P(X= n-k\mid Y=k)\cdot P(Y=k)
  \]
  А вот тут уже поможет независимость $X,Y$.
  \[
    \cdots = P(X=n-k)\cdot P(Y=k)
  \]
\end{itlproof}

\begin{prop}\label{prop:stat::randsum::contsum}
  Пусть $X_1,X_2,n\in\R$~--- две непрерывные независимые случайные величины. Тогда 
  \[
    p_{X_1+X_2}(y) = \int_{\R} p_1(y-t) p_2(t) \,\del t
  \]
\end{prop}
\begin{itlproof}
  Пусть $Y = X_1 + X_2$. Тут видно, что нет биекции, придется руками что-то делать.
  \[
    F_Y(y) = \displaystyle\iint_{x_1+x_2 < y} p(x_1, x_2) \, \del x_1 \del x_2
    = \int_{-\infty}^\infty \del x_1\, \int_{-\infty}^{y-x_1} p(x_1, x_2) \, \del x_2 
    = \int_{-\infty}^\infty \del x_1 \, \int_{-\infty}^y p(x_1, u - x_1) \, \del u
  \]
  Переменные независимы, так что можно поменять местами интегралы (ещё же по области интегрируем,
  неважно как)\note{слишком много раз пользовались на физике, так что оставим на 4 семестр}
  А тогда, убирая внешний интеграл из определения функции распределения, получаем
  \[
    p_Y(y) = \int_{-\infty}^\infty p(t, y-t)\, \del t
  \]
  Поскольку $X_1, X_2$ независимы, то $p(t, y-t) = p_1(t)\cdot p_2(y-t)$. Нумерация никого не
  интересует, так что
  \[
    p_Y(y) =  \int_{-\infty}^\infty p_1(y-t)\cdot p_2(t)\, \del t
  \]
\end{itlproof}

Теперь можно перейти и к содержанию билета
\begin{prop}\label{prop:stat::randsum::exp}
  $\Exp \left(\sum_i X_i\right) = \sum_i \Exp X_i $. Да и вообще оно линейно.
\end{prop}
\begin{itlproof}
  Пусть $f(X,Y)= X+Y$
  \[
    \begin{split}
      \Exp (X+Y) &= \intR f(x,y) p(x,y)\, \del x\del y 
      = \intR x p(x,y)\, \del x \del y + \intR y p(x,y)\, \del x \del y \\
      &= \intR x 
      \left(\intR p(x,y) \del y\right)\, \del x + \intR y \left(\intR p(x,y) \del x\right)\,\del y \\
      &= \Exp X + \Exp Y
    \end{split}
  \]
  Покажем, что $\displaystyle\intR p(x,y) \,\del y = p_X(x)$
  \[
    \begin{split}
      \int_{-\infty}^x \left(\intR p(x,y) \,\del x\right) \, \del y &= F(x, +\infty) = 
      P(X<x, Y<+\infty) \\
      &= P({\omega\mid X(\omega)\in (-\infty, x], Y\in \R\cup\{+\infty\}}) \\
      &= P(X<x) = F_X(x) = \int_{-\infty}^x p_X(x) \, \del x
    \end{split}
  \]
  Опять-таки интервал можно сжать как угодно, правда снова проблемы с непрерывностью.

  Часть про константу слишком очевидна, не будем её доказывать. 
\end{itlproof}

\begin{prop}[Дисперсия суммы]\label{prop:stat::randsum::var}
  $\Var \left(\sum_i X_i\right) = \sum_i \Var X_i$
\end{prop}
\begin{itlproof}
  Сначала заметим, что $\Var X = \Exp (X- \Exp X)^2$, $\Exp (X - \Exp X) = \Exp X - \Exp X = 0$
  \[
    \begin{split}
      \Var (X+Y) &= \Exp \bigl(X+Y - M(X+Y)\bigr)^2 = \Exp \bigl((X-\Exp X) +(Y-\Exp Y) \bigr)^2 \\
                 &= \Exp (X-\Exp X)^2 + \Exp (Y-\Exp Y)^2 + 2 \Exp (X-\Exp X) \Exp (Y - \Exp Y)\\ 
                 &= \Var X + \Var Y
    \end{split}
  \]
\end{itlproof}

\begin{prop}\label{prop:stat::randsum::expmul}
  Если $X,Y$~--- независимы, то $\Exp XY = \Exp X \Exp Y$
\end{prop}
% \begin{itlproof}
%
% \end{itlproof}

\paragraph{Матожидание функции случайной величины}
\label{par:stat::expfun}

\begin{defn}[$\ddot\sim$]\label{defn:stat::expfun::expfun}
  Пусть $f(X)$~--- функция от случаной величины. Тогда 
  $\displaystyle \Exp f(X) = \intR f(x) p(x)\, \del x$. В случае чего он многомерный, просто
  прикидывается. Существует, если есть абсолютная сходимость.
\end{defn}
\begin{rem*}
  я ещё подумаю, может это всё же утверждение.
\end{rem*}
\begin{prop}\label{prop:stat::expfun::lin}
  Матожидание функции линейно
\end{prop}
\begin{prop}\label{prop:stat::expfun::mul}
  Если $X,Y$~--- независимы, то $\Exp f_1(X_1)f_2(X_2) = \Exp f_1(X_1) \Exp f_2(X_2)$
\end{prop}


\paragraph{Неравенство Шварца}
\label{par:stat::shwartz}

\begin{prop}\label{prop:stat::shwartz}
  $(\Exp  XY)^2 \leqslant \Exp X^2 \Exp Y^2$
\end{prop}
\begin{itlproof}
  $\Exp (X+ tY)^2 = t^2\,\Exp Y^2 + 2 t \Exp XY + \Exp X^2 \geqslant 0$ из свойств матожидания. Ну
  там и подынтегральная функция положительна. Тогда квадратное уравнение в правой части может
  иметть не более одного корня.
  \[
    (2 \Exp XY)^2 - 4 \Exp X^2 \Exp Y^2 \leqslant 0 \Leftrightarrow 
    (\Exp  XY)^2 \leqslant \Exp X^2 \Exp Y^2
  \]
\end{itlproof}

\paragraph{Характеристическая функция суммы случайных величин}
\label{par:stat::charfunsum}

\begin{prop}\label{prop:stat::charfunsum::sum}
  Пусть $X,Y$~--- независимые случайные величины. Тогда 
  \[
    \Phi_{X+Y}(t) = \Phi_X(t) \cdot \Phi_Y(t)
  \]
\end{prop}
\begin{itlproof}
  Из~\ref{prop:stat::charfunsum::sum}  
  \[
    \Phi_{X+Y}(t) = \Exp e^{itX} e^{itY} = \Exp e^{itX} \cdot \Exp e^{itY} = \Phi_{X}(t) \cdot
    \Phi_Y (t)
  \]
\end{itlproof}
\begin{cor}\label{conj:stat::charfunsum::sumn}
  Если все величины одинаково распределены, то $\Phi_{X_1 + \dotsb + X_n}(t) =
  \bigl(\Phi(t)\bigr)^n$, \[
    p_{X_1 + \dotsb + X_n} = \frac{1}{2\pi} \intR \bigl(\Phi(t)\bigr)^n \, \del t 
  \]
\end{cor}

\paragraph{Суммирование большого числа случайных величин}
\label{par:stat::randlimsum}
\flame\underdev\sour

\begin{thrm}[ЦПТ Линдберга-Леви-Агекяна \sour]\label{thrm:stat::randlimsum::sum}
  Пусть $X_1, \dotsc, X_n$~--- независимые одинаково распределённые случайные величины. Пусть к
  тому же $S_n = X_1, \dotsc, X_n$, $0 < \Var X_k < \infty$.
  Пусть $\Exp X_k = a, \Var X_k = \sigma$. Тогда при $n\to \infty$ $Z_n \sim N(0,1)$,
  в вариации из Агекяна $S_n \sim N(na,n\sigma^2)$
\end{thrm}
\begin{ittproof}
  Пусть $\Exp X_k = a, \Var X_k = \sigma^2$.
  Рассмотрим характеристическую функцию $\Phi(t) = \Exp e^{itX_k}$. Введём замену (которая
  z-преобразование.):
  \[
    z_n = \frac{S_n - a n}{\sigma \sqrt n}
  \]
  Давайте ещё немного схитрим и положим $X_k \gets X_k- a$. А то потом будет много возни с бедным
  $a$. При этом $z_n = \frac{S_n}{\sigma \sqrt n}$
  Тогда
  \[
    \Phi_{z_n} (t) = \Exp \left(e^{\frac{itS_n}{\sigma\sqrt n}} \right)=
    \left(\Phi\left(\frac{t}{\sigma \sqrt n} \right)\right)^n 
  \]
  А характеристическая функция дифференцируема дважды из существования дисперсии.
  \begin{align*}
    \Phi'(0) &= 0 \\
    \Phi''(0) &= - \sigma^2 \\
    \Phi\left(\frac{t}{\sigma \sqrt n} \right) 
    &= \Phi(0) + \Phi'(0) \frac{t}{\sigma \sqrt n} + \Phi''(0)\, \frac{t^2}{2\sigma^2 n} 
      + o\left(\frac{1}{n}\right)
    = 1  - \frac{1}{2}\,\frac{t^2}{n} +  o\left(\frac{1}{n}\right) \\
  \end{align*}
  А при $n\to \infty$ \[
    \Phi\left(\frac{t}{\sigma \sqrt n} \right)^n 
    = \left(1  - \frac{1}{2}\,\frac{t^2}{n} +  o\left(\frac{1}{n}\right)\right) ^n
    \rightrightarrows e^{-\lfrac{t^2}{2}}
  \]
  Здесь сходимость есть на любом конечном интервале, но вот про всю прямую этого уже не скажешь.
  Так что снова поднимается вопрос какой теоремой о непрерывном соответствии пользоваться.
  Но если ей воспользоваться (тут по-тихому применили обратное преобразование Фурье), то
  \begin{align*}
    p_{z_n}= \frac{1}{2\pi} \intR \exp{\frac{- s^2 + s^2 - 2its - t^2}{2}}  &=
    \frac{e^{-s^2/2}}{\sqrt{2}\pi } 
    \intR \exp\left(-\left(\underbrace{\frac{t+is}{\sqrt 2}}_\eta \right)^2\right) \del \eta 
    =\frac{e^{-s^2/2}}{\sqrt{2\pi} } \\
    F_{Z_n} (x) &= \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-s^2/2} \, \del s 
  \end{align*}

  Дальше~--- вариация из Агекяна.
  Используя утверждение \ref{prop:stat::funcrand::cont} про замену переменной как раз получаем
  нормальное распределение. Только тут нужно поменять в процессе $\sigma$
  \begin{align*}
    Z_n &= \frac{S_n}{\sigma \sqrt n} \\
    F_{S_n} (x) &= \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} \frac{1}{\sigma\sqrt n} 
    e^{-\frac{u^2}{2\sigma^2 n} } \, \del u
  \intertext{Вернёмся обратно к ненулевому $a$ }
    S_n &\gets S_n - n a \\
    F_{S_n} &= \frac{1}{\sqrt{2\pi}} \frac{1}{\sigma\sqrt n}\int_{-\infty}^x  
    e^{-\frac{(u-n a)^2}{2\sigma^2 n} } \, \del u \\
    S_n \sim N (na, n \sigma^2) (n \to \infty)
  \end{align*}
\end{ittproof}


\paragraph{Центральная предельная теорема}
\label{par:stat::cpt}
\begin{thrm}[ЦПТ Ляпунова]\label{thrm:stat::cpt::lyap}
  Пусть $ \{ X_k\}$~--- независимые случаные величины (тут нет одинаковости расределений!).
  Введём гору обозначений:
  \begin{align*}
    S_n &= \sum_i x_i \\
    a_k &= \Exp X_k & \sigma^2_k &= \Var x_k & \gamma_k &= \Exp |X_k - a_k|^3 \\
    A_n &= \sum_{k=1}^n a_k & B_n^2 &= \sum_{k=1}^n \sigma_k^2 & C_n &= \sum_{k=1}^n \gamma_k
  \end{align*}
  Тогда \[
    \frac{C_N}{B_n^3} \xrightarrow[n\to\infty]{} 0  \Rightarrow \frac{S_n - A_n}{B_n}  \xrightarrow[n\to\infty]{d} \mathcal N (0,1)
  \]
\end{thrm}
\begin{rem*}
  Тут какая-то жесть. Она мало где формулируется и нигде не доказывается. Что-то есть
  тут:\cite{msu}, а здесь~\cite{chernova1} так другую теорему обозвали

  \flame
\end{rem*}

\paragraph{Обобщённая теорема Муавра-Лапласа}
\label{par:stat::genmuavr}

\begin{defn}\label{defn:stat::genmuavr::chi}
  Пусть $X_1, \dotsc, X_n \sim \mathcal N (0,1)$ и независимы. Тогда говорят, что случайная
  величина $\displaystyle \chi_n^2 = \sum_{k=1}^n X_k^2$ имеет распределение $\chi^2$ c $n$
  степенями свободы.
\end{defn}
\begin{prop}\label{prop:stat::genmuavr::chi}
  $\displaystyle p_{\chi_b}(z) = \frac{1}{2^{n/2}\cdot \Gamma (\frac{n}{2} )} z^{n/2 -1} e^{-z/2} $
\end{prop}
\begin{itlproof}
  Характеристическая функция $\chi$ может быть найдена из~\ref{conj:stat::charfunsum::sumn}
  Найдём сначала характеристическую функцию $X_k^2$. Для этого было бы недурно найти плотность
  соответвующего распределения
  \[
    \begin{split}
      P(y < X^2 < y+ \del y) &= P(\sqrt y < X < \sqrt{y + \del y})+ P(-\sqrt y>X >-\sqrt{y+\del y})
      \\
      &= \frac{2}{\sqrt{2\pi}}  \int_{\sqrt y}^{\sqrt{y+\del y}} e^{-u^2/2} \, \del u 
      = \sqrt{\frac{2}{\pi} } \frac{e^{-y/2}}{2\sqrt y} 
    \end{split}
  \]
  А теперь можно и фурье-образ найти
  \[
    \intR\frac{e^{-y/2}}{2\sqrt y}\del y = \int_{0}^{+\infty}\frac{e^{-y/2}}{2\sqrt y}\del y
    = \int_{0}^{+\infty} \exp\left(\frac{-\eta^2(1-2it)}{2}\right)\,\del\eta
    = (1-2it)^{-1/2} \, \sqrt{\frac\pi 2}
  \]
  Вспоминая про коэффициент получим $\Phi_k(t) = (1-2it)^{-1/2}$.
  \[
    \Phi_\chi(t) = \left(\Phi_k(t)\right)^n =  (1-2it)^{-n/2}
  \]
  Тогда \[
    p_\chi(z) = \frac{1}{2\pi} \intR (1-2it)^{-n/2}\, e^{-itz}\del t
  \]
  Дальше немного жесть
  \begin{align*}
    \intR (1-2it)^{-n/2}\, e^{-itz}\del t &= \intR e^{-l} \left(1-2\, \frac{l}{z} \right)^{-n/2}
    \frac{1}{iz} \, \del l 
    = 2\cdot\frac{z^{n/2-1} e^{-z/2}}{2^{n/2}} \int_{0}^{\infty} s^{-n/2} e^{-s} \del s \\
    &= 2\frac{z^{n/2-1} e^{-z/2}}{2^{n/2}} \cdot \Gamma\left(1-\frac{n}{2} \right)
  \end{align*}
  Из правила отражения для $\Gamma$-функции 
  $\Gamma(1-n/2)\, \Gamma(n/2) = \frac{\pi}{\sin\frac{\pi n}{2} } $. А это почти что надо.
  \underdev
  там надо интеграл поаккуратнее брать.
\end{itlproof}

\begin{thrm}[Обобщённая теорема Муавра-Лапласа]\label{thrm:stat::genmuavr}
  Пусть $X_1, \dotsc, X_n$~--- независимые случайные величины с дискретным распределением:
  \[
    X_k \colon
    \begin{array}{c|c|c}
      1 & \cdots & r \\ \hline
      p_1 & \cdots & p_r
    \end{array}
  \]
  Рассмотрим $\nu_k = \# \{ 1 \leqslant i \leqslant n \mid X_i =k\},\; 1\leqslant k \leqslant r$.
  Тогда 
  \[
    \sum_{k=1}^r \left( \frac{\nu_k - n p_k}{\sqrt{n p_k}}  \right) \xrightarrow{d} \chi_{r-1}^2
  \]
\end{thrm}

\paragraph{Метод моментов}
\label{par:stat::mom}

Здесь походу нужно все статистические определения в одном параграфе \flame

\subparagraph{Вводные слова}
В отличие от теорвера, матстатистике неизвестно распределение случайной величины. И нужно
придумать, как его восстановить по конкретной реализации. Пусть $X$~--- та самая величина,
про распределение которой очень хочется узнать

Главная героиня матстатистики~--- выборка
\begin{defn}\label{defn:stat::mom::chosen}
  Выборка объёма $n$~--- 
  \begin{enumerate}
    \item $n$ независимых случайных величин, распределённых так же, как и $X$
    \item набор чисел $X_i(\omega)$, $\omega \in \Omega$~--- какой-то исход. Ещё называется
      реализацией выборки.
  \end{enumerate}
  Собственно, первое определение~--- это до испытания, а второе~--- уже после.
\end{defn}

\subparagraph{Основные задачи}
Пусть $X_1, \dotsc, X_n \sim F(x,\theta)$, $\theta \in \Theta \subset \R^d$~--- множество
параметров.

\begin{enumerate}
  \item Оценивание параметров:
    \begin{itemize}
      \item Точечные оценки: $\hat\theta = T(X_1, \dotsc, X_n)$
      \item Доверительные интервалы: $P_\alpha (T_1 < \theta < T_2) = \alpha$
    \end{itemize}
  \item Проверка гипотез\par
    Пусть $\Theta = \Theta_0 \cup \Theta_1$. А мы хотим узнать чему принадлежит $\theta$. 
    \begin{description}
      \item[$H_0$:] $\theta\in \Theta_0$~--- основная гипотеза 
      \item[$H_1$:] $\theta\in \Theta_1$~--- альтернативная гипотеза 
    \end{description}
\end{enumerate}

\subparagraph{Выборочные характеристики}
Выберем реализацию случайной величины (выборку во втором смысле) $X_1, \dotsc, X_n$.
Рассмотрим новую случайную величину:\[
  \widetilde{X} \colon 
  \begin{array}{c|c|c}
    X_1 & \cdots & X_n \\ \hline
    \frac{1}{n} & \cdots & \frac{1}{n}  
  \end{array}
\]

Ну а дальше все выборочные характеристики определяются уже для этой случайной величины.
Напомним следующее
\begin{defn}[Индикатор]\label{defn:stat::mom::ind}
  $\displaystyle I_A(X) = \begin{cases}
    1, &X\in A\\
    0, & X \not\in A
  \end{cases}, 
  I(X < x) = \begin{cases}
    1, &X < x\\
    0, & X \geqslant x
  \end{cases}
  $
\end{defn}
\begin{defn}\label{defn:stat::mom::varser}
  Если $X_1, \dotsc, X_n$ можно упорядочить, то $X_{(1)} \leqslant \cdots \leqslant X_{(n)}$
  называется вариационным рядом.
\end{defn}
{\def\arraystretch{1.5}
\begin{table}[h]
  \raggedleft\noindent
  \begin{tabulary}{1.0\linewidth}{|C|c|c|C|}
    \hline
    \multicolumn{2}{|c|}{\bf Генеральная  совокупность} & \multicolumn{2}{|c|}{\bf Выборка}\\ \hline
    Матожидание & $\Exp X$ & $\overline{X} = \frac{1}{n} \sum_k X_k$ & Выборочное среднее\\
    Дисперсия & $\Var X$ & $S^2 = \frac{1}{n} \sum_k (X_k - \overline X)$ & Выборочная дисперсия\\
    Момент порядка $l$ & $\Exp X^k$ & $m_l = \frac{1}{n} \sum_k X_k^l$ & \\

    Ковариация & $\Exp (X-\Exp X) (Y-\Exp Y)$ & $\frac{1}{n} \sum_k (X_k - \overline X) (Y_k -
    \overline Y)$ & \\
    Ассиметрия ($\gamma_3$) & $\Exp (X- \Exp X)^3/\sigma^3$  
                                      & $\frac 1 n \sum (X- \overline X)^3/S^3$ & \\
    Эксцесс ($\gamma_4$) & $\frac{\Exp (X - \Exp X)^4}{\sigma^4} - 3 $& & \\
    Функция распределения & & $\frac 1 n \sum_k I(X_k < y)$ & эмпирическая \\
    Квантиль порядка $p\in(0;1)$ & $\sup \{x \mid F(x) \leqslant p \}$ & $X_{([pn]+1)}$ 
                                                & член вариационного ряда\\
    \hline
  \end{tabulary}
\end{table}}

\subparagraph{\underdev {Свойства оценок}}
\subparagraph{Метод моментов}

\begin{defn}\label{defn:stat:mom:method}

  Пусть $F(x,\theta)$~--- семейство распределений, $m(x) = \Exp g(x)$~--- какой-то момент этого
  распределения. Пусть известно, что $h(\theta) = m(x)$. Тогда собственно сам метод состоит в том,
  чтобы оценить $\theta$ как решение уравнения выше.
  \[
    \hat \theta = h^{-1} (m(x))
  \]
  В случае чего там векторы, но особо не страшно.

\end{defn}

\begin{exmp}\label{exmp:stat::mom::norm}
  \plholdev{примеры про непрерывные распределения}
\end{exmp}

\paragraph{Метод максимального правдоподобия}
\label{par:stat::maklike}

\begin{defn}\label{defn:stat::maxlike::dens}
  За $p(x,\theta) $ обозначим плотность функции распределения $F(x,\theta)$ в точке $x$ в случае
  непрерывного распределения и $P(X=x)$ в случае дискретного.
\end{defn}
\begin{defn}\label{defn:stat::maxlike::fun}
  Пусть $\{X_k\}$~---$n$ независимых случайных величин. Тогда
  $\displaystyle L(\theta):= \prod_{k=1}^n f_\theta(X_k)$~--- функция правдоподобия.
  Ещё берут её логарифм.
\end{defn}

\begin{defn}\label{defn:stat::maxlike::method}
  Сам метод состоит в следующем:
  \[
    \hat \theta \colon L(\hat \theta) = \max_\theta L (\theta)
  \]
  Однако проще искать максимум у $\ln L(\theta)$. Так можно в силу монотонности логарифма.
  \[
    \bigl( \ln L (\theta)\bigr)' = \frac{L'(\theta)}{L(\theta)} 
  \]
\end{defn}

\plholdev{гора примеров}


\paragraph*{Эффективные оценки}\label{par:stat::eff}

\begin{prop}[Неравенство Рао-Крамера]\label{prop:stat::eff::rao}
  Пусть $\theta, \hat\theta$~--- параметр и его оценка, $b(\theta) = \Exp (\hat\theta -
  \theta)$~--- смещение оценки, $I(\theta)$~--- информация Фишера, $F(x,\theta)$~---
  параметрическое семейство распределений.
  \[
    I(\theta) = \Exp \left( \pder{}{\theta} \ln p(X, \theta)\right)^2,
  \]
  где $p(X,\theta)$ из определения~\ref{defn:stat::maxlike::dens}. Если выполнены условия
  регулярности
  \begin{enumerate}
    \item Существует $C\subset\R \colon \forall\, \theta\in \Theta P(X_1 \in C) =1 $ и $\forall\,
      y \in C \sqrt{p(X, \theta)} \in C^1_\theta (\Theta)$
    \item $I(\theta) \in C_\theta(\Theta)$, $I \geqslant 0$
  \end{enumerate}
  и $\Var \hat \theta $ ограничена на любом компакте $\subset \Theta$, 
  
  то
  \[
    \Exp (\hat\theta - \theta)^2 \geqslant \frac{(1+b'(\theta))}{n I(\theta)} + b^2(\theta) 
  \]
\end{prop}
\begin{rem}\label{rem:stat::eff::reg}
  Всякая регулярность нужна, чтобы можно было законно запихивать производную по параметру по
  интеграл. 
\end{rem}

\begin{defn}\label{defn:stat::eff::eff}
  Оценка называется эффективной, если для неё неравенство Рао-Крамера обращатся в равенство.
\end{defn}

\paragraph{Лемма Фишера}
\label{par:stat::fishlem}

\begin{defn}\label{defn:stat::fishlem::chi}
  Пусть $X_1, \dotsc, X_n$~--- независимые случайные величины с распределением $\mathcal N(0,1)$.
  Тогда $\sum_i X_i^2$ имеет распределение $\chi^2$ c $n$ степенями свободы. Ещё так обозначается:
  $K_n$
\end{defn}
\begin{prop}\label{prop:stat::fishlem::chidens}
  Плотность распределения $\chi^2_n$ ищется по формуле \[\displaystyle k_n = \frac{z^{n/2-1}
  e^{-z/2}}{2^{n/2} \Gamma \left(\frac{n}{2} \right)} \]
\end{prop}

\begin{defn}\label{defn:stat::fishlem::student}
  Пусть $X_1, \dotsc, X_n$~--- независимые случайные величины с распределением $\mathcal N(0,1)$.
  Тогда $\displaystyle \frac{x}{\sqrt{\frac{1}{n} \sum_i X_i^2}}$ имеет распределение Стьюдента 
  c $n$ степенями свободы. 
\end{defn}
\begin{prop}\label{prop:stat::fishlem::studens}
  Плотность распределения $T_n$ ищется по формуле 
  \[\displaystyle t_n = \dfrac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{n}{2}) \sqrt{\pi n}} 
  \left(1+\dfrac {x^2} n\right)^{-\frac{n+1}{2} } \]
\end{prop}


\begin{defn}\label{defn:stat::fishlem::orth}
  Пусть $C\colon V_1 \to V_1$. Тогда $C$~--- ортгональный, если $C C^T = E$
\end{defn}
\begin{cor}\label{cor:stat::fishlem::orth1}
  $\det C = 1$
\end{cor}
\begin{cor}\label{cor:stat::fishlem::orthsavelen}
  $\|Cx\| = \|x\|$
\end{cor}

\begin{prop}\label{prop:stat::fishlem::orthmtx}
  Оператор ортогональный $ \Leftrightarrow $ строки его матрицы 
  (как векторы линейного пространсва наборов чисел) образуют ортонормированный базис.
\end{prop}

\begin{prop}\label{prop:stat::fishlem::orthnorm}
  Пусть $\{X_i\} \sim \mathcal N(0,1)$, $C$~--- ортогональный линейный оператор. 
  Тогда и $Y = CX \sim \mathcal N (0,1)$.
\end{prop}
\begin{itlproof}
  Докажется через утверждение о пребразовании плотности при замене 
  переменных~\ref{prop:stat::funcrand::cont} 
  и следствие~\ref{cor:stat::fishlem::orthsavelen}. Независимость получится просто из того, что
  вышло нормальное распределение. А $\sigma$-у можно сначала засунуть в случайные величины, а
  потом вытащить обратно.
\end{itlproof}

\begin{lem}[Фишера]\label{lem:stat::fishlem::fishlem}
  Пусть $X_1, \dotsc, X_n$ независимы и $X_i \sim \mathcal N (\theta,\sigma^2)$. Тогда
  \begin{enumerate}
    \item 
      $\displaystyle\sqrt n \frac{\overline X - \theta}{\sigma} \sim \mathcal N(0,1)$
    \item $\overline{X}, S^2$ независимы \note{Здесь смещённая дисперсия}
    \item $\displaystyle \frac{n S^2}{\sigma^2} \sim \chi_{n-1}^2 $
    \item $\displaystyle \sqrt{n-1}\, \frac{\overline X - \theta }{S}  \sim t_{n-1}$
  \end{enumerate}
\end{lem}
\begin{ittproof}
  \begin{enumerate}
    \item Заменим $Z_i = \frac{X-\theta }{\sigma}$, $Z_i \sim \mathcal N(0,1)$
      Найдем распределение $\sum_i Z_i= \Sigma$. 
      \begin{align*}
        \Phi_i (t) &= e^{-t^2/2} \\
        \Phi_{S_n} (t) &= \frac 1 n \cdot \exp \left( -n\,\frac{t^2}{2} \right) 
        = \exp \left( -\left(\frac{\sqrt n\, t}{\sqrt 2} \right)^2\right)
        \Rightarrow 
        p_{\Sigma}(z) = \frac{1}{\sqrt{2\pi}\sqrt n} \exp \left( -\frac{x^2}{2 n}\right)\\
        &\frac{S_n - n\theta}{\sigma} \sim \mathcal N (0, \sqrt n) \Rightarrow 
        \frac{n}{\sqrt n} \,  \frac{\av X - \theta}{\sigma} = \sqrt n\,\frac{\av X -\theta}{\sigma}
        \sim \mathcal N(0,1)
      \end{align*}
    \item Пусть
      \[
        C = \begin{pmatrix}
          \frac{1}{\sqrt n} & \cdots & \frac{1}{\sqrt n} \\
          \hdotsfor{3} \\
          \hdotsfor{3}
        \end{pmatrix}
      \]
      Первая строка как вектор имеет норму 1. А значит можно вспомнить процесс Грамма-Шмидта и
      набрать ортонормированный базис. Тогда $C$ будет ортогональным. Пусть $Y = CX$. Из горы
      утверждений выше мы получим:
      \begin{align*}
        Y_1 &= \frac{X_1 + \dotsb + X_n}{\sqrt n} = \av X \sqrt n \\
        \|Y\| &= \|X\| \Rightarrow \sum_i X_i^2 = \sum_i Y_i^2 \\
        S^2 &= \frac{1}{n} \sum_i X_i^2 - (\av X)^2 = \frac{1}{n} \sum_{i=2}^n Y_i
      \end{align*}
      А дальше надо честно посчитать $\cov \left(\frac{Y_1}{\sqrt n}, \sum_{i=2}^n Y_i \right) $.
      Правда ноль получается.
      Если что, \[
        \cov (X,Y) = \Exp (X-\Exp X)(Y - \Exp Y) = \Exp (XY) - \Exp X \Exp Y
      \]
    \item $\displaystyle \frac{n S^2}{\sigma^2}  = \sum_{i=2}^n \left(\frac{Y_i}{\sigma} \right) =
      \chi^2_{n-1}$
    \item Мы там порешили,что $\theta=0$, так что  \[
        \displaystyle \sqrt{n-1} \,\frac{\av X }{S} = \frac{Y_1}{ 
        \sqrt{\frac{n}{n(n-1)}\sum_{i=2}^n Y_i}}=\frac{Y_1}{\sqrt{\frac{1}{n-1}\sum_{i=2}^n Y_i}}
        \sim t_{n-1}
      \]
  \end{enumerate}
\end{ittproof}

\paragraph{Доверительны интервалы нормального распределения}
\label{par:stat::trintnorm}

Здесь собственно перешли к более интересной части~--- от точечных оценок параметров к построению
для доверительных интервалов.

\begin{defn}\label{defn:stat::trintnorm::trint}
  $(T_1, T_2)$~--- доверительный интервал уровня $\gamma$, если $P(T_1 < \theta < T_2) = \gamma$
\end{defn}

Дальше всюду ведутся рассуждения про доверительные интервалы (уровня $\gamma$)
параметров нормального распределения.

\begin{prop}\label{prop:stat::trintnorm::qdispkn}
  Доверительный интервал для $\theta$ при известном $\sigma$ равен 
  $\displaystyle \left( \av X - \sigma \, \frac{z_{(1+\gamma)/2}}{\sqrt n} ; 
  \av X + \sigma \, \frac{z_{(1+\gamma)/2}}{ \sqrt n} \right)$
\end{prop}
\begin{itlproof}
  Интервал ищем явно симметричный, так что посчитаем \[
    P\left(\sqrt n \, \frac{|\av X - \theta|}{\sigma} < z \right) 
  \]
  Поскольку величина внутри подчиняется стандартному нормальному распределению, 
  \[
  P\left(-z < \sqrt n \, \frac{(\av X - \theta)}{\sigma} < z \right) = F_n(z) - F_n(-z) = 
  2 F_n(z) - 1 = \gamma
  \]
  По дороге сделали замену переменной, не пугайтесь.
  А дальше $z = F_n(\frac{1+\gamma}{2} $, что как раз соответствует определению
  $\frac{1+\gamma}{2} $ квантили. Ну а дальше всё уже очевидно из преобразования неравенства
  выше. Мы умеем это делать, можно порассматривать $\{\omega \mid X(\omega) \cdots \}$ как уже
  делали раньше.
\end{itlproof}

\begin{prop}\label{prop:stat::trintnorm::qdispukn}
  Доверительный интервал для $\theta$ при неизвестном $\sigma$ равен 
  $\displaystyle \left( \av X - S \, \frac{t_{n-1,(1+\gamma)/2}}{ \sqrt n} ;
  \av X + S \, \frac{t_{n-1,(1+\gamma)/2}}{\sqrt n} \right)$
\end{prop}
\begin{itlproof}
  аналогично~\ref{prop:stat::trintnorm::dispqkn}, только пользуемся 4 пунктом леммы
  Фишера~\ref{lem:stat::fishlem::fishlem}.
\end{itlproof}

\begin{prop}\label{prop:stat::trintnorm::dispqukn}
  Доверительный интервал для $\sigma^2$ при неизвестном $\theta$ равен 
  $\displaystyle \left( \frac{n S^2}{v^2} ; \frac{n S^2}{u} \right)$. Чиселки $u,v$ определяются
  с помощью $\chi^2$.
\end{prop}

\begin{prop}\label{prop:stat::trintnorm::dispqkn}
  Доверительный интервал для $\sigma^2$ при неизвестном $\theta$ нормально не выражается. Проще
  численно.
  \begin{enumerate}
    \item $\displaystyle \frac{\sum_{i=1}^n (X_i - \theta)^2}{\sigma^2} \sim \chi_n^2 $
    \item $\displaystyle \frac{n\, (\av X - \theta)^2}{\sigma^2} \sim \chi_1^2 $
  \end{enumerate}
\end{prop}

\paragraph{Проверка гипотез по параметрам нормального распределения}
\label{par:stat::hypnorm}

Будем рассматривать здесь простую гипотезу:

\[
  \begin{split}
    H_0 & \colon \theta = \theta_0 \\
    H_1 & \colon \theta \neq \theta_0 
  \end{split}
  \]

\begin{enumerate}
  \item Пусть $X_1, \dotsc, X_n \sim \mathcal N(\theta,\sigma^2)$, $\sigma^2$ известно.
    Примем $H_0 \colon \theta = \theta_0$. Но тогда \[
      \sqrt n\, \frac{\av X -\theta_0}{\sigma}\sim \mathcal N(0,1)
    \]
    из 1 пункта леммы Фишера (\ref{lem:stat::fishlem::fishlem}). 

    Рассмотрим $\alpha$~--- уровень значимости~--- какое-нибудь маленькое число. 
    Часто берут $0.05$. 
    \[
      P\left(\sqrt n\, \frac{|\av X -\theta_0|}{\sigma} > z \right) = \alpha \Leftrightarrow 
      \sqrt n\, \frac{|\av X -\theta_0|}{\sigma} > z_{1-\alpha/2}
    \]
    Таким образом можно найти границы критической области. 

    Если $H_0$ верна, то $\displaystyle P\left(\sqrt n\, \frac{|\av X - \theta|}{\sigma} >
    z_{1-\alpha/2}\right)$ мала. Можно выбрать, меньше чего мы хотим её сделать, и объявить сие
      критерием проверки. Собственно, так и делали на практике.
  \item $\sigma^2$ неизвестна. Здесь всё то же самое, только с распределением Стьюдента. 
\end{enumerate}
А здесь такую
\[
  \begin{split}
    H_0 & \colon \theta_1 = \theta_2 \\
    H_1 & \colon \theta_1 \neq \theta_2 
  \end{split} 
\]
Будем считать, что $X_i$, $Y_i$ независимы, и нормально распеделены:\[
  \begin{split}
    X_1, \dotsc, X_{n_1} \sim \mathcal N (\theta_1, \sigma_1^2) \\
    Y_1, \dotsc, Y_{n_2} \sim \mathcal N(\theta_2, \sigma_2^2) \\
  \end{split}
\]
\begin{enumerate}
  \item $\sigma_1^2, \sigma_2^2$~--- известны.
    \[
      \frac{\av X - \av Y}{\sqrt{\frac{\sigma_1^2}{n_1^2} + \frac{\sigma_2^2}{n_2^2} }} \sim
      \mathcal N (0,1) 
    \]
  \item $\sigma_1^2 = \sigma_2^2$, но не известны
    \[
      \sqrt{\frac{n_1n_2(n_1 + n_2  -2)}{n_1 + n_2} } \cdot \frac{\av X - \av Y}{\sqrt{n_1 S_1^2
      + n_2 S_2^2}} \sim S_{n_1 + n_2 - 2}
    \]
    Это как раз тот самый \texttt{t-тест}
  \item $\sigma_1^2, \sigma_2^2$ неизвестны. Тут вообще ничего не понятно. (Проблемы
    Беренса-Фишера)
\end{enumerate}


\paragraph{Линейная регрессия}
\label{par:stat::regr}


\begin{defn}[Регрессия]\label{defn:stat::regr::reg}
  Пусть $Y, X_1, \dotsc, X_m$~--- случайные векторы. 
  Тогда если определено уравнение $y(x_1, \dotsc, x_m) = \Exp (Y \mid X_1 = x_1, \dotsc, X_m =
  x_m)$,  то $y$ называется регрессией $Y$ по $X_1, \dotsc, X_n$.
\end{defn}

\begin{defn}[Линейная регрессия]\label{defn:stat::regr::lin}
  Пусть $Y, X_1, \dotsc, X_m$~--- случайные векторы. 
  Тогда если определено уравнение $y(x) = \Exp (Y \mid X_i =  x_i \:\forall\, i)$, 
  и $y(x) = x\cdot\theta $ то $y$ называется линейной регрессией $Y$ по $X$.

  Здесь $x$~--- матрица $n\times m$, $\theta\in R^m$, $\epsilon\in \R^n$, $y\in \R^n$
\end{defn}
\begin{rem}\label{rem:stat::regr::funran}
  Можно с тем же успехом написать $Y = y(X)+ \varepsilon$, если $\Exp \varepsilon =0$
\end{rem}

\begin{defn}\label{defn:stat::reg::memb}
  $Y$ называется откликом, $X$~--- регрессоры (предикторы), $\varepsilon$~--- шум, $\theta$~---
  параметры.
\end{defn}

Основной метод поиска оптимальных параметров~--- по  функции максимального правдоподобия. Если не
сильно расписывать, то это выльется в $\argmin\limits_{\hat\theta} (Y-X\hat\theta)^T(Y-X\hat\theta)$

При этом нужны условия Гаусса-Маркова:
\begin{enumerate}
  \item $X^T X$ обратима
  \item $\varepsilon_i \sim \mathcal N(0,\sigma^2)$ и независимы
\end{enumerate}

\begin{prop}\label{prop:stat::reg::form}
  Явное выражение для $\hat\theta$ при минимизации выражения выше выглядит так: 
  $\hat\theta = (X^TX)^{-1} X^T Y$
\end{prop}
\begin{itlproof}
  Пусть \[
    Q(\theta) = (Y-X\theta)^T(Y-X\theta) = Y^T Y - \theta^T X^T Y - Y^T X \theta + 
    \theta^T X^T X \theta  \in \R
  \]
  В координатах это перепишется так (как обычно, суммирование по повторяющимся индексам) \[
    Q(\theta) = y_i y_i - 2\,y_j \theta_i X_{ji}  + (X_{si} \theta_i)\,(X_{sj} \theta_j)
  \]
  Тогда можно и продифференцировать
  \begin{align*}
    \pder{Q}{\theta_j}&= 2 X_{si} X_{sj} \theta_j - 2 X_{ji} y_j = 0 \Leftrightarrow 
    2 X^T X \theta - 2 X^T Y = 0 \Leftrightarrow \hat\theta = (X^T X)^{-1} X^T Y \\
    \pdder{Q}{\theta_j}{\theta_i} &= X_{si} X_{sj} \delta_{ij}
  \end{align*}
  Как видно, там и правда мимимум. Здесь второй дифференциал просто сразу приведён к
  диагональному виду, и все числа на диагонали его матрицы положительны.
\end{itlproof}

\paragraph{Теорема Гаусса-Маркова}
\label{par:stat::gausmark}
\begin{defn}\label{defn:stat::reg::cov2}
  Ковариационная матрица случайных векторов $X$, $Y$~--- матрица ковариаций их компонент
  \[
    \cov (X,Y)_{ij} = \cov (X_i, Y_j) = \Exp (X-\Exp X) (Y - \Exp Y)^T
  \]
\end{defn}
\begin{defn}\label{defn:stat::reg::cov}
  $\cov X = \cov (X,X)$
\end{defn}

\begin{defn}[Эффективность оценок]\label{defn:stat::reg::eff}
  Оценка параметра $\hat\theta_1$ эффективнее оценки $\hat\theta_2$, если матрица $\cov
  \hat\theta_1 - \cov \hat\theta_2$ отрицательно определена.
\end{defn}

\begin{thrm}\label{thrm:stat::guasmark}
  Пусть 
  \begin{enumerate}
    \item $X^T X$ обратима
    \item $\varepsilon_i \sim \mathcal N(0,\sigma^2)$ и независимы
  \end{enumerate}
  Тогда 
  \begin{enumerate}
    \item $\hat \theta = (X^T X)^{-1} X^T Y$~--- несмещённая оценка $\theta$,
    \item $\hat\theta$~--- наиболее эффективная среди линейных несмещённых оценок
  \end{enumerate}
\end{thrm}
\begin{ittproof}
  \begin{enumerate}
    \item $\hat \theta = (X^TX)^{-1} X^T (X\theta + \varepsilon) = \theta + (X^TX)^{-1} X^T
      \varepsilon $. Так как $\Exp \varepsilon = 0$, $\varepsilon$ не зависит от $X$, последнее
      слагаемое обращается в ноль
    \item Пусть $\widetilde{\theta} = HY$. Такая оценка тоже будет несмещённой, если $HX = E$.
      Тогда\[
        \cov HY = (\Exp HY - \theta ) (HY - \theta )^T 
        = \Exp ( (HX-E)\, \theta + \varepsilon)\,( (HX-E)\, \theta + \varepsilon)^T
        = \Exp H \varepsilon \varepsilon^T H^T = \sigma^2 HH^T
      \]
      Для изначальной оценки $H_0 = (X^T X)^{-1}X^T$, так что 
      $H_OH_O^T =(X^T X)^{-1}X^T \, X \, (X^T X)^{-1} = (X^TX)^{-1}$.

      Покажем, что матрица $HH^T - (X^T X)^{-1} $ положительно определена. Пусть 
      $C = H - (X^T X)^{-1}$. Тогда 
      \begin{align*}
        CX &= CH - E = 0\\
        HH^T &= (C + (X^T X)^{-1} X^T)\, (C^T + X\,(X^T X)^{-1}) = CC^T + (X^TX)^{-1}
      \end{align*}
      А матрицы вида $CC^T$ обычно (над $\R$) положительно определены.
  \end{enumerate}
\end{ittproof}

\paragraph{Оценка лисперсии погрешностей}
\label{par:stat::estdispLSM}

\begin{thrm}\label{thrm:stat::estdispLSM::est}
  Пусть $\displaystyle S^2 = \frac{1}{n-m} (Y - X \hat \theta) ( Y - X \hat\theta)^T$. Тогда
  $S^2$~--- несмещённая оценка $\sigma^2$
\end{thrm}

\end{document}
% vim:wrapmargin=3
