\documentclass{trlnotes}
\usepackage{trmath}
\addcompatiblelayout{commonplace}
\setlayout{commonplace}
\usepackage{trthm}
\usepackage{trsym} 
\usepackage{trphys}
\input{mdefs}
\usepackage{silence}
\WarningFilter{latex}{Reference}
\graphicspath{{../../img/}}
\begin{document}

\paragraph{Устойчивость собственных чисел при возмущении матрицы}
\label{par:lin::eigenstab}

Пусть $A$~"--- линейный оператор $\R^s \to \R^s$, $x, b$~"--- векторы-столбцы в $\R^s$.
Здесь будет столько матриц и векторов, что рисовать шляпы не будем, и так понятно 
кто есть кто.

Какие задачи вообще можно здесь решать
\begin{enumerate}
  \item Решение линейной системы $Ax = b$
  \item Поиск собственных чисел $Ax = λx$
\end{enumerate}

Какие при этом могут возникнуть ошибки
\begin{enumerate}
  \item Ошибки округления (алгоритма)
  \item Ошибки начальных данных (неустранимые)
\end{enumerate}

Посмотрим, как оценить ошибки вычисления. 
Пусть $\circ$~--- какая-то операция, а $\circledcirc$~"---  её машинное представление.
Существуют два подхода
\begin{enumerate}
  \item \emph{Прямой анализ ошибок}\par
    Просто учитываем погрешность $a \circ b$ как ошибку округления. Часто
    делают так ($ε_M$~"--- <<машинный эпсилон>>):
    \[
      a \circledcirc b = a \circ b \, (1 + ε), \quad ε \leqslant ε_M
    \]
  \item \emph{Обратный анализ ошибок}(метод эквивалентных возмущений)\par
    Сводим все ошибки к возмущениям начальных данных:
    \[
      a \circledcirc b = \ov~a \circ \ov~b, \quad \ov~a = a + Δa, \; \ov~b = b + Δb.
    \]
    \begin{enumerate}
      \item оцениваем эквивалентные возмущения
      \item оцениваем влияние возмущений
    \end{enumerate}
    получается, что мы все ошибки записали в неустранимые погрешности начальных
    данных
\end{enumerate}

Первый метод частно выдает неправданно большие оценки погрешности,
так что займёмся в основном вторым.

Разберёмся с корректностью задач.
\clause{Решение ЛСУ} ${}$

\begin{defn}[мера обусловленности]\label{defn:lin::eigenstab::condnum}
  $μ = \norm{A}\, \norm{A^{\smash{-1}}}$
\end{defn}
Почему она так выглядит?
Посмотрим какие вообще есть способы оценки вырожденности $A$
\begin{enumerate}
  \item $\det A$. Почти не бывает равным 0. К тому же, перемешивает большие и маленькие 
    собственные числа.
  \item $\dfrac{\norm{Ax}}{\norm{x}}$. Здесь мы пытаемся смотреть на ЛЗ строчек матрицы.
    Но не очень понятно с чем сравнить, чтобы понять близость к ЛЗ. Может быть компоненты
    матрицы маленькие.
  \item $\dfrac{\max \frac{\norm {Ax}}{\norm {x}}}{\min \frac{\norm{Ax}}{\norm{x}}}$ уже выглядит
    разумно. Преобразуем, используя определение нормы (конечномерного) оператора
    \[
      \begin{aligned}
        \max \frac{\norm {Ax}}{\norm {x}} &= \norm{A} \\
        \min \frac{\norm {Ax}}{\norm {x}} &= \min \frac{\norm {y}}{\norm {A^{\smash{-1}}y}} = 
        \frac{1}{\frac{\norm{A^{\smash{-1}}y}}{\norm{y}}} = \norm{A^{{-1}}}^{-1}.
      \end{aligned}
    \]
    А это очень похоже на определение выше.
\end{enumerate}

\begin{lem}\label{lem:lin::eigenstab::idaddinv}
  $\norm{B} < 1\so \exists\,  (I-B)^{-1} \land \norm{(I-B)^{-1}}  \leqslant \dfrac{1}{1 -\norm{B}}$ 
\end{lem}
\begin{prf}
  Рассмотрим систему $x - Bx = y$. Будем искать решение методом простой итерации:
  $x_{n+1} = f(x_n) = B x_n + y$. Покажем, что он сходится. Для этого нужно убедиться что 
  $f$~"---  сжимающее отображение.
  \[
    \norm{f(x) - f(x')} = \norm{B(x-x')} \leqslant \norm{B} \norm{x-x'} < \norm{x-x'}
  \]
  Решение нашлось $\forall\, y \so \exists\, (I-B)^{-1}$.
  Теперь получим оценку нормы
  \[
    \forall\,x \holds x = Bx + y \so \norm{x} \leqslant \norm{B}\norm{x} + \norm{y} 
    \so \norm{x} \leqslant \dfrac{1}{1 -\norm{B}} \norm{y}
  \]
  Тогда это верно и для $\max \norm{x}/\norm{y} = \norm{(I-B)^{-1}}$
\end{prf}

Теперь, оценим, наконец, погрешность решения СЛУ.
\begin{thrm}\label{thrm:lin::eigenstab::stab}
  Рассмотрим возмущенную задачу: $\ov~Ax = \ov~b$.
  Введём относительную и абсолютную погрешность $A, x, b$:
  \[
    \begin{aligned}
      &ΔA = \ov~A - A, &  &Δx = x - x^*, & &Δb = \ov~b - b \\  
      &δ_A = \frac{\norm{ΔA}}{\norm{A}}, & 
      &δ_x = \frac{\norm{Δx}}{\norm{x*}}, &  
      &δ_b = \frac{\norm{Δb}}{\norm{b}} \\  
      x^* &\text{~--- невозмущенное решение} \span\omit\span
    \end{aligned}
  \]
  Тогда
  \[
    δ_x \leqslant \frac{μ(A)}{1-μ(A)δ_A}\, (δ_A + δ_b)
  \]
\end{thrm}
\begin{prf}
  Раз $x^*$~"--- решение $Ax^* = b$, провернём пару скучных манёвров 
  \[
    \begin{aligned}
      A'x = b' &\iff (A + ΔA) \, (x^* + Δx) = b + Δb \iff (A + ΔA)Δx  = -ΔA x^* + Δb \\
               &\iff \Bigl(I - (-A^{-1} ΔA)\Bigr) \,\frac{Δx}{x^*} = -A^{-1}ΔA +
                 A^{-1}\frac{Δb}{x^*}
    \end{aligned}
  \]
  Из леммы выше 
  \[
    \norm{\frac{Δx}{x^*}} \leqslant \frac{1}{1-\norm{A^{\smash{-1}}}\norm{ΔA}}\, 
    \left( \|A^{-1}\|\norm{ΔA} + \|A^{-1}\|\norm{\frac{Δb}{x^*}}\right)
  \]
  Из невозмущённой системы $\norm{x^*} \geqslant \|A\|^{-1}\norm{b}$, вспомнив определение 
  числа обусловленности осознаем $\|A^{-1}\|\norm{ΔA} = μ(A) \, δ_A$.
  Осталось переписать остальное через $δ$ и получить утверждение теоремы.
\end{prf}

\begin{rem}
  Из этой теоремы можно прикинуть ошибку решения ЛСУ. 
  Будем, как и обещали, использовать обратный анализ ошибок.
  Из-за неточного представления в памяти $δ_A, δ_b \sim ε_M$ (ну никак не меньше),
  так что $δ_x \sim C(s)\, μ(A)\, ε_M$‚ $C(s)$~"--- функция параметров задачи.
\end{rem}
\begin{rem}
  На оценку погрешности ещё влияют индивидуальные особенности методов.
  Например, в методе исключения Гаусса часто накапливается ошибка из-за
  деления на маленькие ведущие элементы.
\end{rem}

\clause{Поиск собственных чисел}${}$\\

Некий полезный набор фактов из линейной алгебры, который совсем не
стоит забывать
\begin{enumerate}
  \item $Au - λu$~"---  уравнение на собственные числа и собственные вектора.
  \item $p_A(t) = \det (A -tI)$~"--- характеристический многочлен.
  \item матрицы можно приводить к ЖНФ
  \item ЖНФ~"--- диагональ из жордановых клеток:
    \[
      J_p(a) = \begin{pmatrix}
        a & 1      & \\
          & \ddots & 1\\
          &        & a\\
      \end{pmatrix}\colon\;  p × p, \qquad p_{J_p(a)}(t) = (a-t)^p 
    \]
  \item алгебраическая кратность собственного числа~"--- кратность его как корня
    характеристического многочлена. Совпадает с размерностью корневого
    подпространства ($V(λ)$).
  \item геометрическая кратность~"--- размерность собственного подпространства
    $(V_λ)$.
  \item геометрическая кратность $\leqslant$ алгебраической, 
    ибо $\dim V_λ \leqslant V(λ)$.
  \item собственные числа самосопряженных операторов вещественные.
  \item из собственных векторов самосопряжённого оператора можно собрать ортогональный
    базис.
  \item $\max λ_A = \max \frac {(Au,u)}{(u,u)}$
  \item $\min λ_A = \min \frac {(Au,u)}{(u,u)}$
\end{enumerate}

Теперь наконец обсудим устойчивость
\begin{exmp}\label{exm:lin::eigenstab::unstab}
  Пусть \[
    A=J_p(a),\quad εB \that (εB)_{ij} = δ_{ip} δ_{j1}, \quad \ov~A = A+εB
  \]
  Оценим ошибку собственного числа. Попреобразовываем систему..
  \[
    Ax + εBx = λx \iff \left\{\begin{aligned}
        ax_k + x_{k+1} &= λx_k, & k &\in 1\intrng p-1\\
        εx_1 + ax_p &= λx_p
    \end{aligned}\right.
    \so ε x_1 = (λ-a)^{p}x_1
  \]
  В итоге получается, что $λ = a + ε^{1/s}$\note{можно конечно корень из 1 в $\C$
  посчитать, но идея не изменится}

  Пусть $ε = 10^{-16}$ (удвоенная точность). Тогда уже на матрицах
  порядка $15$ ошибка $\sim 0.1$. Грустная оценка получилась.
\end{exmp}

\paragraph{Теорема Бауэра-Файка}
\label{par:lin::bf}

ситуация немного лучше, когда матрицы симметричные. Можно придумать
не такие грустные оценки как в примере в предыдущем параграфе.

\begin{thrm}\label{thrm:lin::bf}
  Пусть $A$~"--- диагонализуемая матрица, $D^{-1}A D = Λ$. Тогда
  \[
    \abs{λ_{A+B} - λ_A} \leqslant μ(D)\norm{B} 
  \]
\end{thrm}

\end{document}


% vim:wrapmargin=3
